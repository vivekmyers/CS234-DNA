{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "done = 0\n",
    "data = {}\n",
    "for f in listdir('data'):\n",
    "    data[f[:-2]] = pickle.load(open('data/' + f, 'rb'), encoding='latin1')\n",
    "\n",
    "genes = [x for x in data]\n",
    "training = genes[:5]\n",
    "testing = genes[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_p = 1\n",
    "val_n = 0.0\n",
    "\n",
    "pA=[('A' * 20, val_p)],\n",
    "pB=[('T' * 20, val_p)],\n",
    "pC=[('C' * 20, val_p)],\n",
    "pD=[('G' * 20, val_p)],\n",
    "nA=[('A' * 20, val_n)],\n",
    "nB=[('T' * 20, val_n)],\n",
    "nC=[('C' * 20, val_n)],\n",
    "nD=[('G' * 20, val_n)],\n",
    "\n",
    "toy_env = dict(\n",
    "    tA = (5 * nA + 5 * nB + 10 * pA) * 10,\n",
    "    tB = (5 * pC + 5 * pB + 10 * nD) * 10,\n",
    "    tC = (5 * pA + 5 * pD + 5 * nC + 5 * nB) * 10,\n",
    "    tD = (5 * nA + 5 * nB + 10 * pA) * 10,\n",
    "    tE = (5 * pC + 5 * pB + 10 * nD) * 10,\n",
    "    vA = (5 * pA + 5 * pD + 5 * nC + 5 * nB) * 10,\n",
    "    vB = (5 * pA + 5 * pD + 5 * nC + 5 * nB) * 10,\n",
    "    vC = (5 * pC + 5 * pB + 10 * nD) * 10,\n",
    ")\n",
    "data = {}\n",
    "for x in toy_env:\n",
    "    data[x] = [i[0] for i in toy_env[x]]\n",
    "    \n",
    "genes = [x for x in data]\n",
    "training = genes[:5]\n",
    "testing = genes[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "class SeqNet:\n",
    "    def __init__(self, seq_len=20, ktop=5, lr=0.001, gamma=0.8, horizon=10):\n",
    "        self.gamma = gamma\n",
    "        self.seq_len = seq_len\n",
    "        self.ktop = ktop\n",
    "        self.lr = lr\n",
    "        self.horizon = horizon\n",
    "        self.sess = tf.Session()\n",
    "        self.build_placeholders()\n",
    "        self.build_network()\n",
    "        self.make_train_ops()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter('results', self.sess.graph)\n",
    "\n",
    "    def build_placeholders(self):\n",
    "        self.actions = tf.placeholder(tf.float32, shape=[None, self.seq_len, 4], name='actions')\n",
    "        self.sequences = tf.placeholder(tf.float32, shape=[None, self.seq_len * self.ktop, 4], name='sequences')\n",
    "        self.returns = tf.placeholder(tf.float32, shape=[None], name='returns')\n",
    "        self.on_target_labels = tf.placeholder(tf.float32, shape=[None, self.ktop], name='on_target_labels')\n",
    "        self.static_advantages = tf.placeholder(tf.float32, shape=[None], name='static_advantages')\n",
    "\n",
    "    def build_network(self):\n",
    "        self.conv1 = tf.layers.conv1d(inputs=self.sequences, filters=80,\n",
    "                                      kernel_size=7, activation=tf.nn.relu)\n",
    "        self.conv2 = tf.layers.conv1d(inputs=self.conv1, filters=80,\n",
    "                                      kernel_size=7, activation=tf.nn.relu)\n",
    "        self.conv3 = tf.layers.conv1d(inputs=self.conv2, filters=80,\n",
    "                                      kernel_size=1, activation=tf.nn.relu)\n",
    "        self.merged = tf.concat([tf.layers.flatten(self.conv3),\n",
    "                                 self.on_target_labels], axis=1)\n",
    "        self.dense1_actor = tf.layers.dense(inputs=self.merged, units=512,\n",
    "                                            activation=tf.nn.relu)\n",
    "        self.dense2_actor = tf.layers.dense(inputs=self.dense1_actor, units=self.seq_len * 4)\n",
    "        self.dense1_critic = tf.layers.dense(inputs=self.merged, units=512,\n",
    "                                             activation=tf.nn.relu)\n",
    "        self.dense2_critic = tf.layers.dense(self.dense1_critic, units=1)\n",
    "\n",
    "        self.output_seq = tf.reshape(self.dense2_actor, [-1, self.seq_len, 4])\n",
    "        self.sample_output_seq = tf.reshape(tf.one_hot(\n",
    "            tf.reshape(\n",
    "                tf.multinomial(tf.reshape(self.output_seq, [-1, 4]), 1),\n",
    "                shape=[-1, self.seq_len]),\n",
    "            depth=4), [-1, self.seq_len, 4])\n",
    "        self.logprob = tf.reduce_sum(\n",
    "            -tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=self.actions, logits=self.output_seq), axis=1)\n",
    "        self.baseline = tf.squeeze(self.dense2_critic, axis=1)\n",
    "        self.advantage = self.returns - self.baseline\n",
    "\n",
    "    def make_train_ops(self):\n",
    "        self.actor_loss = tf.reduce_mean(-self.logprob * self.static_advantages)\n",
    "        self.train_actor = tf.train.AdamOptimizer(self.lr).minimize(self.actor_loss)\n",
    "        self.critic_loss = tf.reduce_mean(tf.squared_difference(self.returns, self.baseline))\n",
    "        self.train_critic = tf.train.AdamOptimizer(self.lr).minimize(self.critic_loss)\n",
    "\n",
    "    def get_advantages(self, seqs, r_ktops, actions, returns):\n",
    "        return self.sess.run(self.advantage, feed_dict={\n",
    "            self.actions: list(actions),\n",
    "            self.sequences: list(seqs),\n",
    "            self.returns: list(returns),\n",
    "            self.on_target_labels: list(r_ktops),\n",
    "        })\n",
    "\n",
    "    def improve(self, seqs, r_ktop, actions, rewards):\n",
    "        returns = [sum((self.gamma ** (i - j)) * rewards[i]\n",
    "                       for i in range(j, len(rewards)))\n",
    "                   for j, _ in enumerate(rewards)]\n",
    "        adv = self.get_advantages(seqs, r_ktop, actions, returns)\n",
    "        cl, _ = self.sess.run([self.critic_loss, self.train_critic], feed_dict={\n",
    "            self.actions: actions,\n",
    "            self.sequences: seqs,\n",
    "            self.returns: returns,\n",
    "            self.on_target_labels: r_ktop,\n",
    "        })\n",
    "        al, _ = self.sess.run([self.actor_loss, self.train_actor], feed_dict={\n",
    "            self.static_advantages: adv,\n",
    "            self.actions: actions,\n",
    "            self.sequences: seqs,\n",
    "            self.returns: returns,\n",
    "            self.on_target_labels: r_ktop,\n",
    "        })\n",
    "        #print(cl, al)\n",
    "\n",
    "    def run(self, seqs, rewards):\n",
    "        return np.squeeze(self.sess.run(self.sample_output_seq, feed_dict={\n",
    "            self.sequences: np.array([seqs]),\n",
    "            self.on_target_labels: np.array([rewards]),\n",
    "        }))\n",
    "\n",
    "    def flat(self, state):\n",
    "        return np.reshape(state, [state.shape[0] * state.shape[1], 4])\n",
    "\n",
    "    def path(self, samples):\n",
    "        samples = samples[:]\n",
    "        state = random.sample(samples, self.ktop)\n",
    "        for x in state:\n",
    "            for i, v in enumerate(samples):\n",
    "                if (v[0] == x[0]).all():\n",
    "                    del samples[i]\n",
    "                    break\n",
    "        visited = state[:]\n",
    "        path = []\n",
    "        for i in range(self.horizon):\n",
    "            if not samples:\n",
    "                break\n",
    "            seqs, rewards = [np.array(i) for i in zip(*state)]\n",
    "            action = self.run(self.flat(seqs), rewards)\n",
    "            #print(vec_dna(action))\n",
    "            d = lambda x: sum((x[0][i] != action[i]).any() for i, _ in enumerate(x[0]))\n",
    "            new_seq, reward = min(samples, key=d)\n",
    "            for i, v in enumerate(samples):\n",
    "                if (v[0] == new_seq).all():\n",
    "                    del samples[i]\n",
    "                    break\n",
    "            visited.append((new_seq, reward))\n",
    "            path.append((state, action, reward))\n",
    "            state = sorted(visited, key=lambda x: -x[1])[:self.ktop]\n",
    "        return zip(*path)\n",
    "\n",
    "    def train(self, samples, iterations):\n",
    "        for i in range(iterations):\n",
    "            states, actions, rewards = self.path(samples)\n",
    "            seqs = np.array([[v[0] for v in x] for x in states])\n",
    "            labels = np.array([[v[1] for v in x] for x in states])\n",
    "            self.improve(np.reshape(seqs, [seqs.shape[0], \n",
    "                                           seqs.shape[1] * seqs.shape[2], 4]), \n",
    "                                             labels, actions, rewards)\n",
    "            \n",
    "    def evaluate(self, samples, iterations):\n",
    "        rewards = []\n",
    "        for i in range(iterations):\n",
    "            s, a, r = net.path(samples)\n",
    "            for k, i, j in list(zip(s, a, r)):\n",
    "                rewards.append(j)\n",
    "        return np.mean(np.array(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SeqNet(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dna_vec(s):\n",
    "    mask = np.array(['ATCG'.index(i) for i in s])\n",
    "    arr = np.zeros([len(s), 4])\n",
    "    arr[np.arange(len(s)), mask] = 1\n",
    "    return arr\n",
    "\n",
    "def vec_dna(v):\n",
    "    return ''.join(['ATCG'[np.argmax(i)] for i in v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "0.66\n",
      "0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:25<03:52, 25.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "0.83\n",
      "0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:49<03:22, 25.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [01:12<02:52, 24.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [01:35<02:23, 23.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [02:01<02:02, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [02:33<01:47, 27.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [03:07<01:26, 28.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [03:44<01:02, 31.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n",
      "------------------------------\n",
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [04:30<00:35, 35.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in trange(10):\n",
    "    for i in training:\n",
    "        net.train([(dna_vec(a), b) for a, b in data[i]], 10)\n",
    "    print('-' * 30)\n",
    "    for i in testing:\n",
    "        print(net.evaluate([(dna_vec(a), b) for a, b in data[i]], 10))\n",
    "    print('-' * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py3.6env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
